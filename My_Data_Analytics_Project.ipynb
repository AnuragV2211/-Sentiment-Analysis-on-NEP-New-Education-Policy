{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1MSYuKcvxDOuNdAhhhD8LNDZXQ5KzUg9Z",
      "authorship_tag": "ABX9TyNWtgDFMA6aG8DMLY2pkxKM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnuragV2211/AnuragV2211/blob/main/My_Data_Analytics_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic-> Analyzing Public Sentiment Towards the New Education Policy: A Data-Driven Approach**"
      ],
      "metadata": {
        "id": "ViRvglGPw-RW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Program 1-> Sentiment analysis on new education policy text data using a RandomForestClassifier,TF-IDF vectorization and hyperparameter tuning for classification accuracy optimization.**"
      ],
      "metadata": {
        "id": "Z0uJztCNt3Cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = \"/content/drive/MyDrive/My Dataset.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Split dataset into features (text) and target (sentiment)\n",
        "X = df['text']\n",
        "y = df['sentiment']\n",
        "\n",
        "# Preprocess the text data using TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the classifiers to be evaluated\n",
        "classifiers = {\n",
        "    'Random Forest': RandomForestClassifier()\n",
        "}\n",
        "\n",
        "# Perform grid search for hyperparameter tuning\n",
        "best_model = None\n",
        "best_accuracy = 0.0\n",
        "for clf_name, clf in classifiers.items():\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20]\n",
        "    }\n",
        "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the best model on the test set\n",
        "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{clf_name} - Best Accuracy: {grid_search.best_score_}, Test Accuracy: {accuracy}\")\n",
        "\n",
        "    # Save the best model if it has higher accuracy\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "# Save the best model and vectorizer for future use\n",
        "joblib.dump(best_model, 'best_model.pkl')\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "\n",
        "# Function to predict sentiment for new feedback\n",
        "def predict_sentiment(feedback):\n",
        "    # Load the trained model and vectorizer\n",
        "    classifier = joblib.load('best_model.pkl')\n",
        "    vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "    # Preprocess the input feedback\n",
        "    feedback_tfidf = vectorizer.transform([feedback])\n",
        "\n",
        "    # Predict sentiment\n",
        "    sentiment = classifier.predict(feedback_tfidf)\n",
        "\n",
        "    return sentiment[0]\n",
        "\n",
        "# Example usage\n",
        "new_feedback = \"Its emphasis on creativity and critical thinking empowers students to become lifelong learners and problem-solvers.\"\n",
        "predicted_sentiment = predict_sentiment(new_feedback)\n",
        "print(\"Predicted sentiment:\", predicted_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ohv8Cj_Pt8Az",
        "outputId": "eadc01a4-e471-48d4-d806-4d9b991ba382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest - Best Accuracy: 0.7377777777777779, Test Accuracy: 0.6666666666666666\n",
            "Predicted sentiment: neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Program 2-> Sentiment analysis on new education policy text data using a RandomForestClassifier is used in the pipeline, GradientBoostingClassifier, TF-IDF vectorization and incorporating data augmentation through synonym replacement and hyperparameter optimization.**"
      ],
      "metadata": {
        "id": "zYjZ5uCiu15l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S6t78agvMH9",
        "outputId": "c9bac578-fac3-4f83-ef81-151f05d8f7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/My Dataset.csv\")\n",
        "\n",
        "# Data augmentation function using synonym replacement\n",
        "def augment_data(text):\n",
        "    words = text.split()\n",
        "    augmented_texts = []\n",
        "    for i, word in enumerate(words):\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonyms.add(lemma.name())\n",
        "        if synonyms:\n",
        "            synonym = random.choice(list(synonyms))\n",
        "            augmented_texts.append(' '.join(words[:i] + [synonym] + words[i+1:]))\n",
        "    return augmented_texts\n",
        "\n",
        "# Augmenting the data\n",
        "augmented_X = []\n",
        "augmented_y = []\n",
        "for text, label in zip(data['text'], data['sentiment']):\n",
        "    augmented_X.append(text)\n",
        "    augmented_y.append(label)\n",
        "    augmented_texts = augment_data(text)\n",
        "    for augmented_text in augmented_texts:\n",
        "        augmented_X.append(augmented_text)\n",
        "        augmented_y.append(label)\n",
        "\n",
        "# Split augmented data into features (X) and target (y)\n",
        "X = augmented_X\n",
        "y = augmented_y\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=1000)),  # Adjust max_features as needed\n",
        "    ('clf', RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42)) # Optimized hyperparameters\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Function to get sentiment analysis of user input\n",
        "def get_sentiment(text):\n",
        "    prediction = pipeline.predict([text])\n",
        "    return prediction[0]\n",
        "\n",
        "# Get feedback from user\n",
        "new_feedback = \"Its emphasis on creativity and critical thinking empowers students to become lifelong learners and problem-solvers.\"\n",
        "# Perform sentiment analysis\n",
        "sentiment = get_sentiment(new_feedback)\n",
        "\n",
        "# Output sentiment\n",
        "print(\"Sentiment analysis of your feedback:\", sentiment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJmy0eQGu68A",
        "outputId": "83283d4a-ce36-4d53-c06c-f7701a8e7e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9841269841269841\n",
            "Sentiment analysis of your feedback: neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Program 3-> Taking feedback as an input.**"
      ],
      "metadata": {
        "id": "6wnu9cwXvai8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/My Dataset.csv\")\n",
        "\n",
        "# Data augmentation function using synonym replacement\n",
        "def augment_data(text):\n",
        "    words = text.split()\n",
        "    augmented_texts = []\n",
        "    for i, word in enumerate(words):\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonyms.add(lemma.name())\n",
        "        if synonyms:\n",
        "            synonym = random.choice(list(synonyms))\n",
        "            augmented_texts.append(' '.join(words[:i] + [synonym] + words[i+1:]))\n",
        "    return augmented_texts\n",
        "\n",
        "# Augmenting the data\n",
        "augmented_X = []\n",
        "augmented_y = []\n",
        "for text, label in zip(data['text'], data['sentiment']):\n",
        "    augmented_X.append(text)\n",
        "    augmented_y.append(label)\n",
        "    augmented_texts = augment_data(text)\n",
        "    for augmented_text in augmented_texts:\n",
        "        augmented_X.append(augmented_text)\n",
        "        augmented_y.append(label)\n",
        "\n",
        "# Split augmented data into features (X) and target (y)\n",
        "X = augmented_X\n",
        "y = augmented_y\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=1000)),  # Adjust max_features as needed\n",
        "    ('clf', RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42)) # Optimized hyperparameters\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Function to get sentiment analysis of user input\n",
        "def get_sentiment(text):\n",
        "    prediction = pipeline.predict([text])\n",
        "    return prediction[0]\n",
        "\n",
        "# Get feedback from user\n",
        "new_feedback = input(\"Enter your feedback ->\")\n",
        "# Perform sentiment analysis\n",
        "sentiment = get_sentiment(new_feedback)\n",
        "\n",
        "# Output sentiment\n",
        "print(\"Sentiment analysis of your feedback:\", sentiment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "butHN5GkvIIS",
        "outputId": "07e49043-92b2-44ae-cdac-d1e96b66ade7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9920634920634921\n",
            "Enter your feedback -> The feasibility of implementing such a large-scale reform needs careful consideration.\n",
            "Sentiment analysis of your feedback: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zrb2wvxL7EKi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOp0X4JSvqbI",
        "outputId": "a3d5bd93-e2a9-4d6a-ab14-5ada0973117d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fTEZSiN47oiU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}